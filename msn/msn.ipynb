{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Varun\\anaconda3\\envs\\capstone\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    AutoImageProcessor,\n",
    "    ViTModel\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_processor, num_pairs=10000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to folder containing class subfolders.\n",
    "            image_processor: A Hugging Face image processor for ViT.\n",
    "            num_pairs (int): Number of pairs (samples) to generate each epoch.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = image_processor\n",
    "        self.num_pairs = num_pairs\n",
    "        \n",
    "        # Gather all classes (subfolders)\n",
    "        self.classes = [d for d in os.listdir(root_dir) \n",
    "                        if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        \n",
    "        # For each class, gather the list of image paths\n",
    "        self.class_to_images = {}\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            images = [\n",
    "                os.path.join(cls_path, f)\n",
    "                for f in os.listdir(cls_path)\n",
    "                if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "            ]\n",
    "            self.class_to_images[cls] = images\n",
    "        \n",
    "        # Make a list of class names for easy random selection\n",
    "        self.class_list = list(self.class_to_images.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) Randomly pick one class and one image from that class\n",
    "        class1 = random.choice(self.class_list)\n",
    "        img1_path = random.choice(self.class_to_images[class1])\n",
    "        \n",
    "        # 2) Decide if second image is from the same class (label=1) or different (label=0)\n",
    "        if random.random() < 0.5:\n",
    "            class2 = class1\n",
    "            label = 1\n",
    "        else:\n",
    "            class2 = random.choice([c for c in self.class_list if c != class1])\n",
    "            label = 0\n",
    "        \n",
    "        img2_path = random.choice(self.class_to_images[class2])\n",
    "        \n",
    "        # 3) Load images\n",
    "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "        \n",
    "        # 4) Apply the image processor (returns dict with \"pixel_values\")\n",
    "        enc1 = self.image_processor(img1, return_tensors=\"pt\")\n",
    "        enc2 = self.image_processor(img2, return_tensors=\"pt\")\n",
    "        \n",
    "        pixel_values1 = enc1[\"pixel_values\"].squeeze(0)  # (3, H, W)\n",
    "        pixel_values2 = enc2[\"pixel_values\"].squeeze(0)  # (3, H, W)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values1\": pixel_values1,\n",
    "            \"pixel_values2\": pixel_values2,\n",
    "            \"labels\": label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseCollator:\n",
    "    def __call__(self, features):\n",
    "        # features is a list of dicts:\n",
    "        # [\n",
    "        #   {\"pixel_values1\": Tensor, \"pixel_values2\": Tensor, \"labels\": int},\n",
    "        #   {\"pixel_values1\": Tensor, \"pixel_values2\": Tensor, \"labels\": int},\n",
    "        #   ...\n",
    "        # ]\n",
    "        \n",
    "        pixel_values1 = torch.stack([f[\"pixel_values1\"] for f in features], dim=0)\n",
    "        pixel_values2 = torch.stack([f[\"pixel_values2\"] for f in features], dim=0)\n",
    "        labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values1\": pixel_values1,  # (batch_size, 3, H, W)\n",
    "            \"pixel_values2\": pixel_values2,  # (batch_size, 3, H, W)\n",
    "            \"labels\": labels                 # (batch_size,)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseViTMSNConfig(PretrainedConfig):\n",
    "    model_type = \"vit-msn\"\n",
    "\n",
    "    def __init__(self, model_name=\"facebook/vit-msn-base\", embed_dim=256, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model_name = model_name\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "\n",
    "class SiameseViTMSN(PreTrainedModel):\n",
    "    config_class = SiameseViTMSNConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Load base ViT\n",
    "        self.vit = ViTModel.from_pretrained(config.model_name)\n",
    "        \n",
    "        hidden_size = self.vit.config.hidden_size\n",
    "        self.projector = nn.Linear(hidden_size, config.embed_dim)\n",
    "        self.classifier = nn.Linear(config.embed_dim, 1)\n",
    "        \n",
    "        # This is needed for correct saving/loading\n",
    "        self.post_init()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values1=None,\n",
    "        pixel_values2=None,\n",
    "        labels=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        pixel_values1: (batch, 3, H, W)\n",
    "        pixel_values2: (batch, 3, H, W)\n",
    "        labels: (batch,) -> 1 if same class, 0 otherwise\n",
    "        \"\"\"\n",
    "        # Pass each set of images through the ViT\n",
    "        outputs1 = self.vit(pixel_values1)\n",
    "        outputs2 = self.vit(pixel_values2)\n",
    "        \n",
    "        # CLS token is at index 0\n",
    "        cls1 = outputs1.last_hidden_state[:, 0]  # (batch, hidden_size)\n",
    "        cls2 = outputs2.last_hidden_state[:, 0]  # (batch, hidden_size)\n",
    "        \n",
    "        # Project down (optional)\n",
    "        proj1 = self.projector(cls1)  # (batch, embed_dim)\n",
    "        proj2 = self.projector(cls2)  # (batch, embed_dim)\n",
    "        \n",
    "        # Absolute difference for Siamese\n",
    "        diff = torch.abs(proj1 - proj2)  # (batch, embed_dim)\n",
    "        \n",
    "        # Binary logits\n",
    "        logits = self.classifier(diff).squeeze(-1)  # (batch,)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "            # BCE with logits for binary classification\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        \"\"\"\n",
    "        Override to allow a quick creation from a model name without manually\n",
    "        creating the config. We'll create a default config, then load weights.\n",
    "        \"\"\"\n",
    "        config = SiameseViTMSNConfig(model_name=pretrained_model_name_or_path)\n",
    "        model = cls(config)\n",
    "        \n",
    "        # Replace the internal ViT with the pretrained one\n",
    "        model.vit = ViTModel.from_pretrained(pretrained_model_name_or_path)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    eval_pred is a namedtuple: (predictions, label_ids)\n",
    "    - predictions: shape (batch,) -> raw logits\n",
    "    - label_ids: shape (batch,)\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = (1 / (1 + np.exp(-logits))) > 0.5  # sigmoid > 0.5\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type vit_msn to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-msn-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a model of type vit_msn to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/vit-msn-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-msn-base\")\n",
    "    \n",
    "    # 2. Create your train & val datasets\n",
    "train_dataset = SiameseDataset(\n",
    "    root_dir=r\"..\\train-dataset\\train\",\n",
    "    image_processor=image_processor,\n",
    "    num_pairs=8000  # Number of pairs to sample each epoch\n",
    ")\n",
    "val_dataset = SiameseDataset(\n",
    "    root_dir=r\"..\\train-dataset\\train\",\n",
    "    image_processor=image_processor,\n",
    "    num_pairs=2000\n",
    ")\n",
    "data_collator = SiameseCollator()\n",
    "\n",
    "# 4. Load the Siamese model\n",
    "#    This uses the \"facebook/vit-msn-base\" as the underlying ViT.\n",
    "model = SiameseViTMSN.from_pretrained(\"facebook/vit-msn-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./siamese_vitmsn\",\n",
    "     eval_strategy=\"steps\",\n",
    "  num_train_epochs=2,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-5,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to=None,\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# 6. Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 560/2000 09:35 < 24:44, 0.97 it/s, Epoch 0.56/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.703400</td>\n",
       "      <td>0.693854</td>\n",
       "      <td>0.516500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>0.693589</td>\n",
       "      <td>0.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.692900</td>\n",
       "      <td>0.691501</td>\n",
       "      <td>0.505500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>0.693225</td>\n",
       "      <td>0.507500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.694600</td>\n",
       "      <td>0.692387</td>\n",
       "      <td>0.516500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Varun\\anaconda3\\envs\\capstone\\lib\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Varun\\anaconda3\\envs\\capstone\\lib\\site-packages\\transformers\\trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2546\u001b[0m )\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Varun\\anaconda3\\envs\\capstone\\lib\\site-packages\\transformers\\trainer.py:3740\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3738\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\Varun\\anaconda3\\envs\\capstone\\lib\\site-packages\\accelerate\\accelerator.py:2325\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2326\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\Varun\\anaconda3\\envs\\capstone\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Varun\\anaconda3\\envs\\capstone\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Varun\\anaconda3\\envs\\capstone\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
